{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mydevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= MAX_LEN and len(example.tgt) <= MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dummy number reversal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/toy_reverse/train/data.txt'\n",
    "dev_path = 'data/toy_reverse/dev/data.txt'\n",
    "\n",
    "src = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    include_lengths=True\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "\n",
    "data_dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the e2e data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/e2e-dataset/trainset.csv'\n",
    "dev_path = 'data/e2e-dataset/devset.csv'\n",
    "\n",
    "src = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    include_lengths=True\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='csv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "\n",
    "data_dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='csv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens from input vocab:\n",
      " ['Cambridge', 'food[English]', 'Hotel]', 'Vaults],', 'near[Yippee', 'Twenty', 'familyFriendly[no]', 'Noodle', 'Brazil]', 'Sorrento]', 'near[Ranch]', 'than', 'priceRange[cheap]', 'Plough],', 'near[Café', 'One]', 'Arms]', 'Bells]', 'near[Crowne', 'area[city']\n",
      "\n",
      "20 tokens from output vocab:\n",
      " ['Outside', 'Consumers', 'rated', 'describe', 'orientated,', 'places.', 'Twenty', \"'Cocum'.\", 'children,', 'low-prices,', 'okay', 'moderate-', 'pub,', 'sizes', 'fictional', '£20.', 'fare.', 'establishments,', 'lacks', 'wants']\n",
      "\n",
      "num training examples: 42038\n",
      "\n",
      "example train data:\n",
      "src:\n",
      " ['name[The', 'Rice', 'Boat],', 'food[Fast', 'food],', 'priceRange[moderate],', 'customer', 'rating[3', 'out', 'of', '5],', 'area[riverside],', 'familyFriendly[yes],', 'near[Express', 'by', 'Holiday', 'Inn]']\n",
      "tgt:\n",
      " ['<sos>', 'The', 'Rice', 'Boat', 'is', 'a', 'restaurant', 'that', 'serves', 'moderately', 'priced', 'fast', 'food', 'and', 'it', 'is', 'located', 'near', 'Express', 'by', 'Holiday', 'Inn.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "src.build_vocab(data_train, max_size=50000)\n",
    "tgt.build_vocab(data_train, max_size=50000)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "print('20 tokens from input vocab:\\n', list(input_vocab.stoi.keys())[:20])\n",
    "print('\\n20 tokens from output vocab:\\n', list(output_vocab.stoi.keys())[:20])\n",
    "\n",
    "print('\\nnum training examples:', len(data_train.examples))\n",
    "\n",
    "item = random.choice(data_train.examples)\n",
    "print('\\nexample train data:')\n",
    "print('src:\\n', item.src)\n",
    "print('tgt:\\n', item.tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition and training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, myinput, hidden):\n",
    "        embedded = self.embedding(myinput).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)\n",
    "\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
    "          max_length=MAX_LEN, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    # get an initial hidden state for the encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # zero the gradients of the optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # get the seq lengths, used for iterating through encoder/decoder\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # create empty tensor to fill with encoder outputs\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "    # create a variable for loss\n",
    "    loss = 0\n",
    "    \n",
    "    # pass the inputs through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # create a start-of-sequence tensor for the decoder\n",
    "    decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "    # set the decoder hidden state to the final encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # decide if we will use teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "                \n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "        \n",
    "        if decoder_input.item() == output_vocab.stoi[EOS_TOKEN]:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01, teacher_forcing_ratio=0.5):\n",
    "    print('Running {} epochs'.format(n_iters))\n",
    "    print_loss_total = 0\n",
    "    print_loss_epoch = 0\n",
    "\n",
    "    encoder_optim = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optim = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_iterator = torchtext.data.Iterator(\n",
    "        dataset=data_train, batch_size=1,\n",
    "        sort=False, sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device=mydevice, repeat=False)\n",
    "    \n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(n_iters):\n",
    "        batch_generator = batch_iterator.__iter__()\n",
    "        step = 0\n",
    "        start = time.time()\n",
    "        for batch in batch_generator:\n",
    "            step += 1\n",
    "            \n",
    "            # get the input and target from the batch iterator\n",
    "            input_tensor, input_lengths = getattr(batch, 'src')\n",
    "            target_tensor = getattr(batch, 'tgt')\n",
    "            \n",
    "            input_tensor = input_tensor[0]\n",
    "            target_tensor = target_tensor[0]\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, criterion, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            print_loss_total += loss\n",
    "            print_loss_epoch += loss\n",
    "            \n",
    "\n",
    "            if step % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                t = (time.time() - start) / 60\n",
    "                print('step: {}\\t avg loss: {:2f}\\t time for {} steps: {:2f} min'.format(step,print_loss_avg,print_every,t))\n",
    "                start = time.time()\n",
    "        \n",
    "        print_loss_avg = print_loss_epoch / step\n",
    "        print_loss_epoch = 0\n",
    "        print('End of epoch {}, avg loss {:2f}'.format(e,print_loss_avg)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "encoder1 = EncoderRNN(len(input_vocab), hidden_size).to(mydevice)\n",
    "decoder1 = DecoderRNN(hidden_size, len(output_vocab)).to(mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainIters(encoder1, decoder1, 1, print_every=1000, teacher_forcing_ratio=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(146, 128)\n",
      "  (gru): GRU(128, 128)\n",
      ")\n",
      "DecoderRNN(\n",
      "  (embedding): Embedding(4905, 128)\n",
      "  (gru): GRU(128, 128)\n",
      "  (out): Linear(in_features=128, out_features=4905, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#torch.save(encoder1.state_dict(),'encoder.mdl')\n",
    "#torch.save(decoder1.state_dict(),'decoder.mdl')\n",
    "encoder1.load_state_dict(torch.load('encoder.mdl'))\n",
    "decoder1.load_state_dict(torch.load('decoder.mdl'))\n",
    "print(encoder1)\n",
    "print(decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LEN):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            next_word = output_vocab.itos[topi.item()]\n",
    "            decoded_words.append(next_word)\n",
    "            if next_word == EOS_TOKEN:\n",
    "                break\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(candidate, kVal):\n",
    "        candidates = []\n",
    "        decoder_input = Variable(torch.LongTensor([[candidate[0][-1]]]))\n",
    "        if torch.cuda.is_available():\n",
    "            decoder_input = decoder_input.cuda()\n",
    "        sequence, decoder_hidden, encoder_outputs = candidate\n",
    "        decoder_output, decoder_hidden = decoder1(decoder_input, decoder_hidden)\n",
    "\n",
    "        topk = decoder_output.data.topk(kVal)\n",
    "        for k in range(kVal):\n",
    "            topk_prob = topk[0][0][k]\n",
    "            topk_index = int(topk[1][0][k])\n",
    "            candidates.append([sequence+[topk_index],  decoder_hidden, encoder_outputs])\n",
    "        return candidates\n",
    "    \n",
    "def beamSearch(encoder, decoder, sentence, k, max_length=MAX_LEN):\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        \n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        # get the top candidates of beam size\n",
    "        topk = decoder_output.data.topk(k)\n",
    "        candidates = [[] for i in range(k)]\n",
    "        dead_k = 0\n",
    "        final_candidates = []\n",
    "        for index in range(k):\n",
    "            topk_prob = topk[0][0][index]\n",
    "            topk_index = int(topk[1][0][index])\n",
    "            candidates[index] = [[topk_index], decoder_hidden, encoder_outputs]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            tmp = []\n",
    "            for index in range(len(candidates)):\n",
    "                tmp.extend(infer(candidates[index], k))\n",
    "            candidates = []\n",
    "\n",
    "            df = pd.DataFrame(tmp)\n",
    "            df.columns = ['sequence', \"decoder_hidden\", \"encoder_outputs\"]\n",
    "            sequence_len = df.sequence.apply(lambda x:len(x))\n",
    "            df = df[:(k-dead_k)]\n",
    "            for index in range(len(df)):\n",
    "                group = df.ix[index]\n",
    "                \n",
    "                # EOS TOKEN\n",
    "                if group.tolist()[0][-1] == 3:\n",
    "                    final_candidates.append(group.tolist())\n",
    "                    df = df.drop([index], axis=0)\n",
    "                    dead_k += 1\n",
    "\n",
    "            candidates = df.values.tolist()\n",
    "            if len(candidates) == 0:\n",
    "                break  \n",
    "        \n",
    "        if len(final_candidates) < k:\n",
    "            final_candidates.extend(candidates[:(k-dead_k)])\n",
    "            \n",
    "        return final_candidates   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name[The', 'Twenty', 'Two],', 'food[French],', 'area[city', 'centre],', 'familyFriendly[yes]']\n",
      "\n",
      "<sos> The Twenty Two is a French restaurant in the city centre is <eos>\n",
      "\n",
      "<sos> The Twenty Two is a French restaurant in the city centre is and <eos>\n",
      "\n",
      "<sos> The Twenty Two is a French restaurant in the city centre is and is <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kVal = 3\n",
    "\n",
    "item = random.choice(data_train.examples)\n",
    "seq = item.src\n",
    "candidates = beamSearch(encoder1,decoder1, seq, kVal)\n",
    "\n",
    "print(seq)\n",
    "print()\n",
    "for candidate in candidates:\n",
    "    outstrs = []\n",
    "    for i in candidate[0]:\n",
    "        outstrs.append(output_vocab.itos[i])\n",
    "    print(\" \".join(outstrs))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name[Loch', 'Fyne],', 'eatType[restaurant],', 'food[Fast', 'food],', 'familyFriendly[yes]']\n",
      "<sos> Loch Fyne is a fast food restaurant that is kid friendly and has a price range of £20-25. <eos>\n",
      "\n",
      "['name[Loch', 'Fyne],', 'eatType[restaurant],', 'food[French],', 'familyFriendly[no]']\n",
      "<sos> Loch Fyne is a French restaurant that is not not family-friendly. <eos>\n",
      "\n",
      "['name[The', 'Waterman],', 'food[French],', 'priceRange[more', 'than', '£30],', 'customer', 'rating[5', 'out', 'of', '5],', 'area[city', 'centre],', 'familyFriendly[yes]']\n",
      "<sos> The Waterman is a French restaurant in the city centre with a price range of of of the city and a customer rating of 3 out of 5. <eos>\n",
      "\n",
      "['name[The', 'Rice', 'Boat],', 'food[Japanese],', 'priceRange[more', 'than', '£30],', 'customer', 'rating[5', 'out', 'of', '5],', 'area[riverside],', 'familyFriendly[yes],', 'near[Express', 'by', 'Holiday', 'Inn]']\n",
      "<sos> The Rice Boat is a restaurant The Rice Boat located near Express by Holiday of Holiday of the Holiday of 5 and the of Holiday of out of 5. of <eos>\n",
      "\n",
      "['name[The', 'Vaults],', 'food[French],', 'priceRange[£20-25],', 'familyFriendly[yes]']\n",
      "<sos> The Vaults is a restaurant The French restaurant with a price range of £20-25. <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    item = random.choice(data_train.examples)\n",
    "    seq = item.src\n",
    "    print(seq)\n",
    "    #print(\" \".join(item.tgt[1:-1]))\n",
    "    words = evaluate(encoder1, decoder1, seq)\n",
    "    print(' '.join(words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk import ngrams\n",
    "\n",
    "def brevity(canidate,references):\n",
    "    clen = len(canidate)\n",
    "    rlens=[]\n",
    "    # get lengths of references\n",
    "    for i in references:\n",
    "        rlens.append(len(i))\n",
    "    \n",
    "    tmp = list(dict.fromkeys(rlens))\n",
    "\n",
    "    diff=[]\n",
    "    # get the differences\n",
    "    for i in tmp:\n",
    "        diff.append((i,abs(clen-i)))\n",
    "    \n",
    "    # sort on the differences\n",
    "    diff.sort(key = lambda x: x[1])\n",
    "\n",
    "    # if the diff is 0, return candidate lengths\n",
    "    for i in diff:\n",
    "        if i[1] == 0:\n",
    "            return clen, clen\n",
    "    \n",
    "    # find the closest\n",
    "    closest = diff[0][0]\n",
    "    closest_len= diff[0][1]\n",
    "    anotherClosest = False\n",
    "    for item in diff:\n",
    "        if item[0] != closest and item[1]==closest_len:\n",
    "            anotherClosest = True\n",
    "            closest2 = item\n",
    "            \n",
    "    if anotherClosest == False:\n",
    "        return closest,clen\n",
    "     \n",
    "    return (closest, clen) if closest < closest2[0] else (closest2[0],clen)\n",
    "\n",
    "def getNgramPrec(candidate, references, n):\n",
    "    \n",
    "    # tokenize candidate\n",
    "    tcandidate = candidate\n",
    "    grams = ngrams(tcandidate, n)\n",
    "    candidate=[]\n",
    "    \n",
    "    if n == 1:\n",
    "        candidate = tcandidate\n",
    "    else:\n",
    "        for i in grams:\n",
    "            candidate.append(i)\n",
    "            \n",
    "    clen = len(candidate)\n",
    "    \n",
    "    # if len is 0, back off to the previous n\n",
    "    x = 1\n",
    "    newN = n\n",
    "    while clen == 0:\n",
    "        if n - 1 == 0:\n",
    "            return 0.0\n",
    "        grams=ngrams(tcandidate,n - x)\n",
    "        candidate=[]\n",
    "        if n == 1:\n",
    "            candidate = tcandidate\n",
    "        else:\n",
    "            for i in grams:\n",
    "                candidate.append(i)\n",
    "        clen = len(candidate)\n",
    "        newN = n - x\n",
    "        x += 1\n",
    "        \n",
    "    count={}\n",
    "    for i in candidate:\n",
    "        key=i\n",
    "        counts=candidate.count(key)\n",
    "        if n ==1:\n",
    "            count[key]=counts\n",
    "        else:\n",
    "            count[','.join(key)]=counts\n",
    "\n",
    "    # tokenize references\n",
    "    trefs = references\n",
    "    \n",
    "    rgrams=[]\n",
    "    for i in trefs:\n",
    "        tmp = ngrams(i,n)\n",
    "        tmpLst=[]\n",
    "        for i in tmp:\n",
    "            tmpLst.append(i)\n",
    "        rgrams.append(tmpLst)\n",
    "    \n",
    "    # count min\n",
    "    minctr={}\n",
    "    for i in rgrams:\n",
    "        for token in i:\n",
    "            counts=i.count(token)\n",
    "        \n",
    "            key=','.join(token)\n",
    "        \n",
    "            if key not in minctr:\n",
    "                minctr[key]=counts\n",
    "            else:\n",
    "                if minctr[key] < counts:\n",
    "                    minctr[key] = counts\n",
    "                    \n",
    "    # get modified ngrams       \n",
    "    total = []\n",
    "    for i,j in count.items():\n",
    "        if i not in minctr:\n",
    "            ctr = 0\n",
    "        else:\n",
    "            ctr = minctr[i]\n",
    "        w = (i, min(j, ctr))\n",
    "        total.append(w)\n",
    "    \n",
    "    score=0\n",
    "    \n",
    "    for i in total:\n",
    "        w = i[1]\n",
    "        score = score + w\n",
    "    return score / clen, newN\n",
    "\n",
    "def makeDict(datas):\n",
    "    d = {}\n",
    "    for data in datas:\n",
    "        tmp = str(data.src)\n",
    "        if tmp not in d:\n",
    "            d[tmp] = []\n",
    "            d[tmp].append(data.src)\n",
    "        \n",
    "        sentence = data.tgt[1:-1]\n",
    "        # strip '.'\n",
    "        sentence = [s.strip('.') for s in sentence]\n",
    "        d[tmp].append(sentence)\n",
    "    return d\n",
    "\n",
    "def getBleu(canidate, references, n):\n",
    "    precisions=[]\n",
    "    # sum up wn * logpn\n",
    "    for i in range(1, n+1):\n",
    "        precision,newN= getNgramPrec(canidate, references, i)\n",
    "        if precision != 0.0:\n",
    "            precisions.append(1/n*math.log(precision))\n",
    "    \n",
    "    # get brecity penalty\n",
    "    reflen,canlen = brevity(canidate,references)\n",
    "\n",
    "    # apply brevity penalty\n",
    "    if canlen > reflen:\n",
    "        return math.exp(sum(precisions))\n",
    "    else:\n",
    "        return math.exp(1-(reflen/canlen)) * math.exp(sum(precisions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My BLEU-4 score with beam size 3 -> 0.45566752275121164\n",
      "My BLEU-4 score with greedy -> 0.4108040888998508\n"
     ]
    }
   ],
   "source": [
    "bres = 0.0\n",
    "gres = 0.0\n",
    "myDict = makeDict(data_dev)\n",
    "for data in myDict:\n",
    "    seq = myDict[data][0]\n",
    "    beam = beamSearch(encoder1, decoder1, seq, 3)\n",
    "    outstrs = []\n",
    "    # select the top-scoring sequence output\n",
    "    for idx in beam[0][0]:\n",
    "        # remove EOS/SOS tokens\n",
    "        if idx == 4:\n",
    "            continue\n",
    "        if idx == 3:\n",
    "            break\n",
    "        outstrs.append(output_vocab.itos[idx])\n",
    "    tmp = getBleu(outstrs, myDict[data][1:],4)\n",
    "    bres += tmp\n",
    "    \n",
    "for data in myDict:\n",
    "    seq = myDict[data][0]\n",
    "    words = evaluate(encoder1, decoder1, seq)\n",
    "    tmp = getBleu(words,myDict[data][1:],4)\n",
    "    gres += tmp \n",
    "print(\"My BLEU-4 score with beam size 3 -> \" + str(bres/len(myDict)))\n",
    "print(\"My BLEU-4 score with greedy -> \" + str(gres/len(myDict)))\n",
    "# The output of this cell should be the average BLEU score on the dev set\n",
    "# for greedy decoding AND for beam search decoding (beam size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK BLEU-4 score with beam size 3 -> 0.45384270523296344\n",
      "NLTK BLEU-4 score with greedy -> 0.4089792713816026\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nbres = 0.0\n",
    "ngres = 0.0\n",
    "w = [(1/3,1/3,1/3),(1/2,1/2)]\n",
    "beamSize = 3\n",
    "for data in myDict:\n",
    "    seq = myDict[data][0]\n",
    "    words = evaluate(encoder1, decoder1, seq)\n",
    "    tmp = nltk.translate.bleu_score.sentence_bleu(myDict[data][1:],words)\n",
    "    trial = 0\n",
    "    while tmp == 0.0:\n",
    "        tmp = nltk.translate.bleu_score.sentence_bleu(myDict[data][1:],words, weights=w[trial])\n",
    "        trial += 1\n",
    "        if trial > 1:\n",
    "            break\n",
    "    ngres += tmp \n",
    "    \n",
    "for data in myDict:\n",
    "    seq = myDict[data][0]    \n",
    "    beam = beamSearch(encoder1, decoder1, seq, beamSize)\n",
    "    outstrs = []\n",
    "    # select the top-scoring sequence output\n",
    "    for idx in beam[0][0]:\n",
    "        # remove EOS/SOS tokens\n",
    "        if idx == 4:\n",
    "            continue\n",
    "        if idx == 3:\n",
    "            break\n",
    "        outstrs.append(output_vocab.itos[idx])\n",
    "    tmp = nltk.translate.bleu_score.sentence_bleu(myDict[data][1:],outstrs)\n",
    "    trial = 0\n",
    "    while tmp == 0.0:\n",
    "        tmp = nltk.translate.bleu_score.sentence_bleu(myDict[data][1:],outstrs, weights=w[trial])\n",
    "        trial += 1\n",
    "        if trial > 1:\n",
    "            break\n",
    "    nbres += tmp    \n",
    "    \n",
    "print(\"NLTK BLEU-4 score with beam size 3 -> \" + str(nbres / len(myDict)))\n",
    "print(\"NLTK BLEU-4 score with greedy -> \" + str(ngres / len(myDict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
